{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1716043732954,
     "user": {
      "displayName": "Joyce Lai",
      "userId": "17564261997700311548"
     },
     "user_tz": -480
    },
    "id": "inBdhVEcLSE-",
    "outputId": "35159984-031a-491f-ef62-6beaa5b7fa38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 394066 Chinese sentences and 394066 English sentences.\n"
     ]
    }
   ],
   "source": [
    "# 在Google Colab中运行时，请确保上传文件到适当的目录\n",
    "zh_file_path = '/mnt/raw.zh'\n",
    "en_file_path = '/mnt/raw.en'\n",
    "\n",
    "# 读取数据\n",
    "with open(zh_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_zh = f.readlines()\n",
    "\n",
    "with open(en_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_en = f.readlines()\n",
    "\n",
    "print(f\"Loaded {len(raw_zh)} Chinese sentences and {len(raw_en)} English sentences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 21051,
     "status": "ok",
     "timestamp": 1716043898837,
     "user": {
      "displayName": "Joyce Lai",
      "userId": "17564261997700311548"
     },
     "user_tz": -480
    },
    "id": "yr-1sprvLVxz"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 创建分词器\n",
    "tokenizer_zh = Tokenizer(filters='')\n",
    "tokenizer_en = Tokenizer(filters='')\n",
    "\n",
    "# 训练分词器\n",
    "tokenizer_zh.fit_on_texts(raw_zh)\n",
    "tokenizer_en.fit_on_texts(raw_en)\n",
    "\n",
    "# 将文本转换为序列\n",
    "seqs_zh = tokenizer_zh.texts_to_sequences(raw_zh)\n",
    "seqs_en = tokenizer_en.texts_to_sequences(raw_en)\n",
    "\n",
    "# 填充序列\n",
    "seqs_zh = pad_sequences(seqs_zh, padding='post')\n",
    "seqs_en = pad_sequences(seqs_en, padding='post')\n",
    "\n",
    "# 创建训练和测试数据\n",
    "train_size = int(0.8 * len(seqs_zh))\n",
    "X_train, X_test = seqs_zh[:train_size], seqs_zh[train_size:]\n",
    "y_train, y_test = seqs_en[:train_size], seqs_en[train_size:]\n",
    "\n",
    "# 将标签进行one-hot编码\n",
    "vocab_size_en = len(tokenizer_en.word_index) + 1\n",
    "y_train = pad_sequences(y_train, padding='post', value=0)\n",
    "y_test = pad_sequences(y_test, padding='post', value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1716043905501,
     "user": {
      "displayName": "Joyce Lai",
      "userId": "17564261997700311548"
     },
     "user_tz": -480
    },
    "id": "ZUIaO5gwL1ly"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size=16):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(X))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X_batch = self.X[indices]\n",
    "        y_batch = self.y[indices]\n",
    "        decoder_input_data = np.zeros_like(y_batch)\n",
    "        decoder_input_data[:, 1:] = y_batch[:, :-1]\n",
    "        decoder_target_data = y_batch[:, :, np.newaxis]\n",
    "        return [X_batch, decoder_input_data], decoder_target_data[:, :, 0]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6136127,
     "status": "ok",
     "timestamp": 1716051303981,
     "user": {
      "displayName": "Joyce Lai",
      "userId": "17564261997700311548"
     },
     "user_tz": -480
    },
    "id": "dfAC-U0dQhdB",
    "outputId": "e281f0d8-00af-4287-8d70-e34006ac5493"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_zh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-20982d021155>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m  \u001b[1;31m# 减小嵌入维度大小\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m  \u001b[1;31m# 减少LSTM单元数量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mvocab_size_zh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer_zh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# 编码器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer_zh' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "\n",
    "# 模型参数\n",
    "embedding_dim = 8  # 减小嵌入维度大小\n",
    "units = 8  # 减少LSTM单元数量\n",
    "vocab_size_zh = len(tokenizer_zh.word_index) + 1\n",
    "\n",
    "# 编码器\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(input_dim=vocab_size_zh, output_dim=embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# 解码器\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(input_dim=vocab_size_en, output_dim=embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_en, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 构建模型\n",
    "rnn_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn_model.summary()\n",
    "\n",
    "# 创建数据生成器\n",
    "train_generator = DataGenerator(X_train, y_train, batch_size=16)\n",
    "val_generator = DataGenerator(X_test, y_test, batch_size=16)\n",
    "\n",
    "# 训练模型\n",
    "rnn_model.fit(train_generator, validation_data=val_generator, epochs=1)  # 进一步减少训练轮数以加快训练速度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "executionInfo": {
     "elapsed": 3835,
     "status": "error",
     "timestamp": 1716069837943,
     "user": {
      "displayName": "Joyce Lai",
      "userId": "17564261997700311548"
     },
     "user_tz": -480
    },
    "id": "7Kcm4CHrkWPJ",
    "outputId": "09ae5ff5-269f-4fdc-9ceb-20b00cce35f7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D, Dense\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = inputs\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    return LayerNormalization(epsilon=1e-6)(x + res)\n",
    "\n",
    "def build_transformer_model(input_shape, vocab_size_zh, vocab_size_en, num_heads=4, ff_dim=32, num_transformer_blocks=2):\n",
    "    encoder_inputs = Input(shape=input_shape)\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    # 编码器部分\n",
    "    x = Embedding(vocab_size_zh, embedding_dim)(encoder_inputs)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size=embedding_dim, num_heads=num_heads, ff_dim=ff_dim)\n",
    "    encoder_outputs = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # 解码器部分\n",
    "    y = Embedding(vocab_size_en, embedding_dim)(decoder_inputs)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        y = transformer_encoder(y, head_size=embedding_dim, num_heads=num_heads, ff_dim=ff_dim)\n",
    "\n",
    "    # 解码器的输出\n",
    "    decoder_outputs = Dense(vocab_size_en, activation=\"softmax\")(y)\n",
    "\n",
    "    return Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 构建Transformer模型\n",
    "input_shape = (X_train.shape[1],)\n",
    "transformer_model = build_transformer_model(input_shape, vocab_size_zh, vocab_size_en)\n",
    "transformer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_model.summary()\n",
    "\n",
    "# 训练Transformer模型\n",
    "transformer_model.fit(train_generator, validation_data=val_generator, epochs=3)  # 进一步减少训练轮数以加快训练速度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1lMf_xjvgh8"
   },
   "outputs": [],
   "source": [
    "# 使用模型生成伪标签\n",
    "pseudo_labels = transformer_model.predict(X_test)\n",
    "pseudo_labels = np.argmax(pseudo_labels, axis=-1)\n",
    "\n",
    "# 将原始英文标签和伪标签合并\n",
    "X_combined = np.concatenate((X_train, X_test), axis=0)\n",
    "y_combined = np.concatenate((y_train, pseudo_labels), axis=0)\n",
    "\n",
    "# 创建数据生成器\n",
    "combined_generator = DataGenerator(X_combined, y_combined, batch_size=16)\n",
    "\n",
    "# 重新训练模型\n",
    "transformer_model.fit(combined_generator, epochs=3, validation_data=val_generator)  # 进一步减少训练轮数以加快训练速度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0y3Vz0IkdWT"
   },
   "outputs": [],
   "source": [
    "# 评估RNN Seq2Seq模型\n",
    "rnn_test_loss, rnn_test_acc = rnn_model.evaluate(val_generator)\n",
    "print(f\"RNN Seq2Seq Test accuracy: {rnn_test_acc}\")\n",
    "\n",
    "# 评估Transformer模型\n",
    "transformer_test_loss, transformer_test_acc = transformer_model.evaluate(val_generator)\n",
    "print(f\"Transformer Test accuracy: {transformer_test_acc}\")\n",
    "\n",
    "# 演示翻译结果\n",
    "def translate_sentence(sentence, model, tokenizer_input, tokenizer_output):\n",
    "    seq = tokenizer_input.texts_to_sequences([sentence])\n",
    "    seq = pad_sequences(seq, maxlen=X_train.shape[1], padding='post')\n",
    "    prediction = model.predict([seq, seq])\n",
    "    predicted_seq = np.argmax(prediction, axis=-1)\n",
    "    translated_sentence = tokenizer_output.sequences_to_texts(predicted_seq)\n",
    "    return translated_sentence[0]\n",
    "\n",
    "# 示例翻译\n",
    "example_sentence = \"你好，世界！\"\n",
    "rnn_translated_sentence = translate_sentence(example_sentence, rnn_model, tokenizer_zh, tokenizer_en)\n",
    "transformer_translated_sentence = translate_sentence(example_sentence, transformer_model, tokenizer_zh, tokenizer_en)\n",
    "\n",
    "print(f\"RNN Seq2Seq Translated: {rnn_translated_sentence}\")\n",
    "print(f\"Transformer Translated: {transformer_translated_sentence}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPZrlgnAmZIRqF/rTNwRXFj",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
